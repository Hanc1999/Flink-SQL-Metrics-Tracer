{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da8a63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee89bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = '/home/flink/workspace/yimin/datasets/nexmark_full_job_Nov14/test/test2.csv'\n",
    "# ametrics = None\n",
    "# with open(f_path) as csv_file:\n",
    "#     csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "#     line_count = 0\n",
    "#     for row in csv_reader:\n",
    "#         if line_count == 0:\n",
    "#             print(row)\n",
    "#             line_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ca5e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3af164e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequential_cleaner:\n",
    "    def __init__(self, wanted_dict):\n",
    "        self.wanted_run = wanted_dict.get('run_confs', None)\n",
    "        self.wanted_job = wanted_dict.get('job_id', None)\n",
    "        self.wanted_job_meta = wanted_dict.get('job_meta_info', None)\n",
    "        self.wanted_job_level = wanted_dict.get('job_level_metrics', None) \n",
    "        self.wanted_vertex = wanted_dict.get('vertex_id', None)\n",
    "        self.wanted_vertex_level = wanted_dict.get('vertex_level_metrics', None)\n",
    "        self.wanted_sb_level = wanted_dict.get('sub_level_metrics', None)\n",
    "\n",
    "        self.head = self.make_head()\n",
    "        self.dataset = []\n",
    "    \n",
    "    def make_head(self,):\n",
    "        res = []\n",
    "        if self.wanted_run:\n",
    "            res += self.wanted_run\n",
    "        if self.wanted_job:\n",
    "            res += ['job_id']\n",
    "        if self.wanted_job_meta:\n",
    "            res += self.wanted_job_meta\n",
    "        if self.wanted_job_level:\n",
    "            res += self.wanted_job_level\n",
    "        if self.wanted_vertex:\n",
    "            if self.wanted_vertex == 'idx':\n",
    "                res += ['vertex_idx']\n",
    "            else:\n",
    "                res += ['vertex_id']\n",
    "        if self.wanted_vertex_level:\n",
    "            res += self.wanted_vertex_level\n",
    "        if self.wanted_sb_level:\n",
    "            res += ['sub_idx']\n",
    "            res += self.wanted_sb_level\n",
    "        return res\n",
    "    \n",
    "    def get_head(self,):\n",
    "        return self.head\n",
    "\n",
    "#     def help_select_wanted(self, attr_dict, want_list, sub_key = 'value'):\n",
    "#         res = []\n",
    "#         feasible_counter = 0\n",
    "#         for k in attr_dict.keys():\n",
    "#             if k in want_list:\n",
    "#                 feasible_counter += 1\n",
    "#                 if sub_key:\n",
    "#                     res.append(attr_dict[k][sub_key])\n",
    "#                 else:\n",
    "#                     res.append(attr_dict[k])\n",
    "#         if feasible_counter < len(want_list):\n",
    "#             # means not all wanted is here, so skip\n",
    "#             return []\n",
    "#         return res\n",
    "    \n",
    "    def help_select_wanted(self, attr_dict, want_list, sub_key = 'value'):\n",
    "        res = []\n",
    "        feasible_counter = 0\n",
    "        for k in want_list:\n",
    "            if k in attr_dict.keys():\n",
    "                feasible_counter += 1\n",
    "                if sub_key:\n",
    "                    res.append(attr_dict[k][sub_key])\n",
    "                else:\n",
    "                    res.append(attr_dict[k])\n",
    "            else:\n",
    "                break\n",
    "        if feasible_counter < len(want_list):\n",
    "            # means not all wanted is here, so skip\n",
    "            return []\n",
    "        return res\n",
    "    \n",
    "    def make_run_attributes(self, run_confs):\n",
    "        return self.help_select_wanted(run_confs, self.wanted_run, '')\n",
    "    \n",
    "    def make_job_meta_attributes(self, job_metrics):\n",
    "        return self.help_select_wanted(job_metrics['job_meta_info'], self.wanted_job_meta, '')\n",
    "    \n",
    "    def make_job_level_attributes(self, job_metrics):\n",
    "        # TODO: needs further config info\n",
    "        res = []\n",
    "        job_metrics['job_level_metrics']\n",
    "        for k,v in job_metrics.items():\n",
    "            if k in self.wanted_job_level:\n",
    "                res.append(max([e['value'] for e in v.values()]))\n",
    "        return res\n",
    "    \n",
    "    def make_ver_level_attributes(self, job_metrics):\n",
    "        dicts1 = job_metrics['job_meta_info']['vertices']\n",
    "        dicts2 = job_metrics['vertice_metrics']\n",
    "        v_num = len(dicts1)\n",
    "        res1 = [self.help_select_wanted(dicts1[i], self.wanted_vertex_level, '') for i in range(v_num)]\n",
    "        res2 = [self.help_select_wanted(dicts2[i]['vertex_level_metrics'], self.wanted_vertex_level, 'avg') for i in range(v_num)]\n",
    "        res = []\n",
    "        for i in range(v_num):\n",
    "            res.append(res1[i] + res2[i])\n",
    "        return res\n",
    "    \n",
    "    def make_sb_level_attributes(self, vertice_metrics):\n",
    "        res = []\n",
    "        for vm in vertice_metrics:\n",
    "            v_res = []\n",
    "            for sb_idx in range(len(vm['sub_level_metrics'])):\n",
    "                v_res.append([sb_idx] + self.help_select_wanted(vm['sub_level_metrics'][sb_idx], self.wanted_sb_level))\n",
    "            res.append(v_res)\n",
    "        return res # 3-d array\n",
    "    \n",
    "    def assemble_rows(self, sub_rows_1=[], sub_rows_2=[], sub_rows_3=[]):\n",
    "        # TODO: risky function, not robust yet\n",
    "        res = [] # collected samples\n",
    "        \n",
    "        sub_rows_1_combined = []\n",
    "        for r in sub_rows_1:\n",
    "            sub_rows_1_combined += r\n",
    "        \n",
    "        sub_rows_2_combined = []\n",
    "        for i in range(len(sub_rows_2[0] if sub_rows_2 else [])): # v num\n",
    "            tmp = []\n",
    "            for r in sub_rows_2:\n",
    "                tmp += r[i]\n",
    "            sub_rows_2_combined.append(tmp)\n",
    "        \n",
    "        # sub_rows_3_combined do not need to re-assemble\n",
    "        \n",
    "        # assumes there must be sub-task level metrics: (or, sub_rows_3 is not empty)\n",
    "        for i in range(len(sub_rows_3)): # i->vertex\n",
    "            for j in range(len(sub_rows_3[i])): # j->subtask\n",
    "                a_sample = []\n",
    "                a_sample += sub_rows_1_combined\n",
    "                if sub_rows_2_combined:\n",
    "                    sbr = sub_rows_2_combined[i]\n",
    "                    if not sbr: continue\n",
    "                    a_sample += sbr\n",
    "                sbr = sub_rows_3[i][j]\n",
    "                if not sbr: continue\n",
    "                a_sample += sbr\n",
    "                res.append(a_sample)\n",
    "        return res # return a set of tabular samples\n",
    "    \n",
    "    def clean_row(self, row):\n",
    "        run_confs = eval(row['run_confs']) # dict\n",
    "        job_metrics = eval(row['job_metrics']) # dict\n",
    "        \n",
    "        cleaned_rows = []\n",
    "        \n",
    "        sub_rows_1 = [] # 1d\n",
    "        sub_rows_2 = [] # 2d\n",
    "        sub_rows_3 = [] # 3d\n",
    "        \n",
    "        if self.wanted_run:\n",
    "            run_attrs = self.make_run_attributes(run_confs) # 1-list\n",
    "            sub_rows_1.append(run_attrs)\n",
    "        if self.wanted_job:\n",
    "            job_attrs = [job_metrics['job_id']]\n",
    "            sub_rows_1.append(job_attrs)\n",
    "        if self.wanted_job_meta:\n",
    "            job_meta_attrs = self.make_job_meta_attributes(job_metrics)\n",
    "            sub_rows_1.append(job_meta_attrs)\n",
    "        if self.wanted_job_level:\n",
    "            job_level_attrs = self.make_job_level_attributes(job_metrics)\n",
    "            sub_rows_1.append(job_level_attrs)\n",
    "        \n",
    "        if self.wanted_vertex:\n",
    "            if self.wanted_vertex == 'idx':\n",
    "                ver_attrs = [[i] for i in range(len(job_metrics['vertice_metrics']))]\n",
    "            else:\n",
    "                ver_attrs = [[e['vertex_id']] for e in job_metrics['vertice_metrics']]\n",
    "            sub_rows_2.append(ver_attrs)\n",
    "        if self.wanted_vertex_level:\n",
    "            ver_level_attrs = self.make_ver_level_attributes(job_metrics)\n",
    "            sub_rows_2.append(ver_level_attrs)\n",
    "\n",
    "        if self.wanted_sb_level:\n",
    "            sb_level_attrs = self.make_sb_level_attributes(job_metrics['vertice_metrics'])\n",
    "            sub_rows_3.append(sb_level_attrs)\n",
    "\n",
    "        # debug\n",
    "#         print(sub_rows_3[0], '\\n')\n",
    "        res = self.assemble_rows(sub_rows_1, sub_rows_2, sub_rows_3[0])\n",
    "#         print(res, '\\n\\n')\n",
    "        self.dataset += res\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "526ffebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 0, 2, 0, 'NaN', '0', '0', '0.0', '0.0', '0.0'],\n",
       " [9, 0, 2, 1, 'NaN', '0', '0', '0.0', '0.0', '0.0'],\n",
       " [9, 1, 2, 0, '957.0', '21', '0', '0.0', '1334750.1333333333', '5562.4'],\n",
       " [9,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  '679.0',\n",
       "  '4',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '1330381.0666666667',\n",
       "  '5527.916666666667'],\n",
       " [9,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  '0.0',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '2603979.3666666667',\n",
       "  '3908.516666666667'],\n",
       " [9,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  '0.0',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '2478367.183333333',\n",
       "  '3688.7833333333333'],\n",
       " [9, 0, 2, 0, 'NaN', '0', '0', '0.0', '0.0', '0.0'],\n",
       " [9, 0, 2, 1, 'NaN', '0', '0', '0.0', '0.0', '0.0'],\n",
       " [9, 1, 2, 0, '919.0', '21', '0', '0.0', '3356821.283333333', '13984.7'],\n",
       " [9, 1, 2, 1, '975.0', '4', '0', '0.0', '3338385.9166666665', '13879.65']]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted_dict = {}\n",
    "wanted_dict['run_confs'] = ['qid']\n",
    "# wanted_dict['job_id'] = 1\n",
    "wanted_dict['vertex_level_metrics'] = ['parallelism',]\n",
    "wanted_dict['vertex_id'] = 'idx'\n",
    "wanted_dict['sub_level_metrics'] = ['busyTimeMsPerSecond', 'Shuffle.Netty.Input.Buffers.inputQueueLength',\n",
    "                                    'Shuffle.Netty.Input.Buffers.inputQueueSize', 'Shuffle.Netty.Input.Buffers.inPoolUsage',\n",
    "                                    'numBytesInPerSecond', 'numRecordsInPerSecond',]\n",
    "scleaner = sequential_cleaner(wanted_dict)\n",
    "df[150:160].apply(scleaner.clean_row, axis=1)\n",
    "scleaner.dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9b995696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qid',\n",
       " 'vertex_id',\n",
       " 'parallelism',\n",
       " 'sub_idx',\n",
       " 'busyTimeMsPerSecond',\n",
       " 'Shuffle.Netty.Input.Buffers.inputQueueLength',\n",
       " 'Shuffle.Netty.Input.Buffers.inputQueueSize',\n",
       " 'Shuffle.Netty.Input.Buffers.inPoolUsage',\n",
       " 'numBytesInPerSecond',\n",
       " 'numRecordsInPerSecond']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scleaner.get_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "61621839",
   "metadata": {},
   "outputs": [],
   "source": [
    "debugg = eval(df['job_metrics'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7a02918b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpointStartDelayNanos': {'value': '0'},\n",
       " 'Shuffle.Netty.Output.Buffers.outPoolUsage': {'value': '0.0076923077'},\n",
       " 'numBytesInLocal': {'value': '191413'},\n",
       " 'checkpointAlignmentTime': {'value': '0'},\n",
       " 'numBytesInRemotePerSecond': {'value': '0.0'},\n",
       " 'Shuffle.Netty.Input.numBytesInRemotePerSecond': {'value': '0.0'},\n",
       " 'maxSoftBackPressureTimeMs': {'value': '0'},\n",
       " 'numBytesOut': {'value': '1540096'},\n",
       " 'buffers.inputQueueSize': {'value': '0'},\n",
       " 'buffers.outputQueueSize': {'value': '1468'},\n",
       " 'numBytesIn': {'value': '2625806'},\n",
       " 'numBuffersOut': {'value': '47'},\n",
       " 'Shuffle.Netty.Input.numBuffersInLocal': {'value': '258'},\n",
       " 'numBuffersInRemotePerSecond': {'value': '0.0'},\n",
       " 'numBytesOutPerSecond': {'value': '19660.8'},\n",
       " 'buffers.outputQueueLength': {'value': '2'},\n",
       " 'numBuffersOutPerSecond': {'value': '0.6'},\n",
       " 'Shuffle.Netty.Input.Buffers.inputExclusiveBuffersUsage': {'value': '0.0'},\n",
       " 'isBackPressured': {'value': 'false'},\n",
       " 'softBackPressuredTimeMsPerSecond': {'value': '0'},\n",
       " 'numBytesInLocalPerSecond': {'value': '2589.95'},\n",
       " 'Shuffle.Netty.Output.Buffers.outputQueueSize': {'value': '1632'},\n",
       " 'buffers.inPoolUsage': {'value': '0.0'},\n",
       " 'idleTimeMsPerSecond': {'value': '867'},\n",
       " 'Shuffle.Netty.Input.numBytesInLocalPerSecond': {'value': '2589.95'},\n",
       " 'Shuffle.Netty.Input.Buffers.inputQueueSize': {'value': '0'},\n",
       " 'numBytesInRemote': {'value': '0'},\n",
       " 'hardBackPressuredTimeMsPerSecond': {'value': '0'},\n",
       " 'Shuffle.Netty.Input.numBytesInLocal': {'value': '191413'},\n",
       " 'busyTimeMsPerSecond': {'value': '133.0'},\n",
       " 'Shuffle.Netty.Input.numBytesInRemote': {'value': '0'},\n",
       " 'Shuffle.Netty.Output.Buffers.outputQueueLength': {'value': '2'},\n",
       " 'buffers.inputFloatingBuffersUsage': {'value': '0.0'},\n",
       " 'currentInputWatermark': {'value': '1668529949051'},\n",
       " 'Shuffle.Netty.Input.Buffers.inPoolUsage': {'value': '0.0'},\n",
       " 'maxHardBackPressureTimeMs': {'value': '0'},\n",
       " 'numBuffersInLocalPerSecond': {'value': '3.5'},\n",
       " 'numRecordsOut': {'value': '37945'},\n",
       " 'numBuffersInLocal': {'value': '258'},\n",
       " 'numBuffersInRemote': {'value': '0'},\n",
       " 'buffers.inputQueueLength': {'value': '4'},\n",
       " 'numRecordsIn': {'value': '63177'},\n",
       " 'Shuffle.Netty.Input.numBuffersInRemote': {'value': '0'},\n",
       " 'numBytesInPerSecond': {'value': '34438.26666666667'},\n",
       " 'backPressuredTimeMsPerSecond': {'value': '0'},\n",
       " 'Shuffle.Netty.Input.Buffers.inputQueueLength': {'value': '4'},\n",
       " 'buffers.inputExclusiveBuffersUsage': {'value': '0.0'},\n",
       " 'Shuffle.Netty.Input.numBuffersInRemotePerSecond': {'value': '0.0'},\n",
       " 'numRecordsOutPerSecond': {'value': '489.75'},\n",
       " 'buffers.outPoolUsage': {'value': '0.0076923077'},\n",
       " 'Shuffle.Netty.Input.numBuffersInLocalPerSecond': {'value': '3.5'},\n",
       " 'numRecordsInPerSecond': {'value': '828.2166666666667'},\n",
       " 'Shuffle.Netty.Input.Buffers.inputFloatingBuffersUsage': {'value': '0.0'}}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debugg['vertice_metrics'][1]['sub_level_metrics'][1]\n",
    "# debugg['job_meta_info']['vertices'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05c42306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min': 0.0, 'max': 0.0, 'avg': 0.0, 'sum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(debugg['vertice_metrics'][i]['vertex_level_metrics']['numBytesInPerSecond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4ae12586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['1']+[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "16317117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if 'Nan':\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffdbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
